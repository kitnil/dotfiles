#!/usr/bin/env bash

if [[ -z "$1" ]]
then
    viddy -n "${KUBECTL_WATCH_INTERVAL:-10}" "kubectl get nodes -o wide; echo; kubectl get all -A -o wide; echo; kubectl get ingress -o wide; echo; kubectl get secrets -o wide; echo; kubectl get all -n kube-system -A -o wide"
else
    namespace="$1"
    export namespace
    viddy_command()
    {
        case "$namespace" in
            pdns)
                kustomization="powerdns"
                ;;
            kube-system)
                kustomization="cilium"
                ;;
            *)
                kustomization="$namespace"
                ;;
        esac
        if [[ $(kubectl get -n "$namespace" scaledobjects -o json | jq '.items | length') -gt 0 ]]
        then
            cat <<EOF
$ kubectl get -n "$namespace" scaledobjects
$(kubectl get -n "$namespace" scaledobjects)

EOF
        fi
        case "$KUBECONFIG" in
            *config-mjru-cluster1)
                PROMETHEUS_URL=https://prometheus.corp1.majordomo.ru
                OPENSEARCH_ENDPOINT="https://opensearch.corp1.majordomo.ru:9200"
                OPENSEARCH_PASSWORD="$(pass show majordomo/public/opensearch-dashboards/admin)"
                ALERTMANAGER_URL="https://alertmanager.corp1.majordomo.ru"
                ;;
            *config-mjru-cluster2)
                PROMETHEUS_URL=https://prometheus.corp2.majordomo.ru
                OPENSEARCH_ENDPOINT="https://opensearch.corp2.majordomo.ru:9200"
                OPENSEARCH_PASSWORD="$(pass show majordomo/public/opensearch-dashboards/admin)"
                ALERTMANAGER_URL="https://alertmanager.corp2.majordomo.ru"
                ;;
            *config-home-k8s)
                PROMETHEUS_URL=https://prometheus.home.wugi.info
                OPENSEARCH_ENDPOINT="https://node-0.example.com:9200"
                OPENSEARCH_PASSWORD="$(pass show localhost/opensearch-dashboards/admin)"
                ALERTMANAGER_URL="https://alertmanager.home.wugi.info"
                ;;
        esac
        case "$KUBECONFIG" in
            *config-mjru-*)
                cat <<EOF
$ git ls-remote https://gitlab.corp1.majordomo.ru/cd/fluxcd.git master
$(git ls-remote https://gitlab.corp1.majordomo.ru/cd/fluxcd.git master)

EOF
                ;;
            *config-home-*)
                cat <<EOF
$ git ls-remote https://gitlab.com/wigust/dotfiles.git master
$(git ls-remote https://gitlab.com/wigust/dotfiles.git master)

EOF
                ;;
        esac
        if kubectl get -n "$namespace" scaledobjects &> /dev/null
        then
            mapfile -t queries < <(kubectl get -n "$namespace" scaledobjects "${namespace}-scaledobject" -o json | jq --raw-output '.spec.triggers[] | .metadata.query')
            for query in "${queries[@]}"
            do
                cat <<EOF
query: ${query}
$(echo "$query" | curl --silent --get "${PROMETHEUS_URL}/api/v1/query" --data-urlencode query@-)

EOF
            done
        fi
        cat <<EOF
$ kubectl get -n flux-system gitrepositories.source.toolkit.fluxcd.io flux-system
$(kubectl get -n flux-system gitrepositories.source.toolkit.fluxcd.io flux-system)

EOF
        if kubectl get -n flux-system kustomizations.kustomize.toolkit.fluxcd.io "$kustomization" &> /dev/null
        then
            cat <<EOF
$ kubectl get -n flux-system kustomizations.kustomize.toolkit.fluxcd.io "$kustomization"
$(kubectl get -n flux-system kustomizations.kustomize.toolkit.fluxcd.io "$kustomization")

EOF
        fi
        mapfile -t helmreleases < <(kubectl -n "$namespace" get helmreleases.helm.toolkit.fluxcd.io --no-headers=true --output=custom-columns='NAME:metadata.name')
        for helmrelease in "${helmreleases[@]}"
        do
            if kubectl -n "$namespace" get helmreleases.helm.toolkit.fluxcd.io "$helmrelease" &> /dev/null
            then
                cat <<EOF
$ kubectl -n "$namespace" get helmreleases.helm.toolkit.fluxcd.io "$helmrelease"
$(kubectl -n "$namespace" get helmreleases.helm.toolkit.fluxcd.io "$helmrelease")

EOF
            fi
        done
        if [[ $namespace == "kube-system" ]]
        then
            mapfile -t nodes_with_noschedule_taint < <(kubectl get nodes -o go-template='{{range $item := .items}}{{with $nodename := $item.metadata.name}}{{range $taint := $item.spec.taints}}{{if and (eq $taint.effect "NoSchedule")}}{{printf "%s\n" $nodename}}{{end}}{{end}}{{end}}{{end}}' | sort --version-sort)
            nodes_with_noschedule()
            {
                for node in "${nodes_with_noschedule_taint[@]}"
                do
                    echo -n "${node}: "
                    kubectl get node -o json "$node" \
                        | jq --monochrome-output --compact-output '.metadata.labels | with_entries(select(.key | test("^.*cluster.local.*$"))) | with_entries(select(.value == "false")) | keys'
                done
            }
            cat <<EOF
$ kubectl get nodes -o wide
$(kubectl get node -o wide | awk 'NR<3{print $0;next}{print $0| "sort --version-sort"}')

$ kubectl get nodes ... # with NoSchedule taint
$(nodes_with_noschedule)

$ kubectl top node
$(kubectl top node | awk 'NR<3{print $0;next}{print $0| "sort --version-sort"}')

$ kubectl cluster-info
$(kubectl cluster-info)

$ kubectl get --raw='/readyz?verbose'
$(kubectl get --raw='/readyz?verbose')

EOF
        fi
        if [[ $namespace == "flux-system" ]]
        then
            cat <<EOF
$ kubectl get --namespace flux-system kustomizations ... # suspended
$(kubectl get --namespace flux-system kustomizations.kustomize.toolkit.fluxcd.io --output=json | jq --raw-output '.items[] | select(.spec.suspend == true) | .metadata.name')

$ kubectl get --all-namespaces helmreleases ... # suspended
$(kubectl get --all-namespaces helmreleases.helm.toolkit.fluxcd.io --output=json | jq --raw-output '.items[] | select(.spec.suspend == true) | .metadata.name'
tf-controller)

$ kubectl get --all-namespaces kustomizations.kustomize.toolkit.fluxcd.io
$(kubectl get --all-namespaces kustomizations.kustomize.toolkit.fluxcd.io)

$ kubectl get --all-namespaces helmreleases.helm.toolkit.fluxcd.io
$(kubectl get --all-namespaces helmreleases.helm.toolkit.fluxcd.io)

EOF
        fi

        if [[ $namespace == "hms-vm-"* ]]
        then
            mongo_id="$(echo ${namespace/hms-vm-/})"
            cat <<EOF
$ curl "https://api.majordomo.ru/virtual-private-server/debug-kubernetes/${mongo_id}"
$(bash -ic "curl 'https://api.majordomo.ru/virtual-private-server/debug-kubernetes/${mongo_id}'")
EOF
        fi

        cat <<EOF
$ kubectl top --namespace "$namespace" pod
$(kubectl top --namespace "$namespace" pod)

$ kubectl get all --namespace "$namespace" -o wide
$(kubectl get all --namespace "$namespace" -o wide)

$ kubectl get endpoints --namespace "$namespace" -o wide
$(kubectl get endpoints --namespace "$namespace" -o wide)

$ kubectl get ingress --namespace "$namespace" -o wide
$(kubectl get ingress --namespace "$namespace" -o wide)

EOF
        if kubectl get -n "$namespace" ingress &> /dev/null
        then
            mapfile -t hosts < <(kubectl -n "$namespace" get ingress -o json | jq --raw-output '.items[] | .spec.rules[] | .host' | sort -u)
            for host in "${hosts[@]}"
            do
                query="probe_http_status_code{instance=\"https://${host}/\"}"
                cat <<EOF
query: ${query}
$(echo "$query" | curl --silent --get "${PROMETHEUS_URL}/api/v1/query" --data-urlencode query@-)

EOF
            done
        fi
        cat <<EOF
$ amtool --alertmanager.url="$ALERTMANAGER_URL" alert query --output=simple namespace="$namespace"
$(amtool --alertmanager.url="$ALERTMANAGER_URL" alert query --output=simple namespace="$namespace")

$ kubectl get --namespace "$namespace" certificates -o wide
$(kubectl get --namespace "$namespace" certificates -o wide)

$ kubectl get secrets --namespace "$namespace" -o wide
$(kubectl get secrets --namespace "$namespace" -o wide)

$ kubectl get --namespace "$namespace" serviceaccounts
$(kubectl get --namespace "$namespace" serviceaccounts)

$ kubectl get pvc --namespace "$namespace" -o wide
$(kubectl get pvc --namespace "$namespace" -o wide)

EOF

        query="kubelet_volume_stats_available_bytes{job=\"kubelet\", metrics_path=\"/metrics\", namespace=\"$namespace\"}"
        cat <<EOF
query: ${query}
$(echo "$query" | curl --silent --get "${PROMETHEUS_URL}/api/v1/query" --data-urlencode query@- | jq --raw-output '.data.result[] | .value[1]' | numfmt --to=iec-i --suffix=B | xargs echo)

$ kubectl get networkpolicies --namespace "$namespace" -o wide
$(kubectl get networkpolicies --namespace "$namespace" -o wide)

$ kubectl get events --namespace "$namespace" -o wide
$(kubectl get events --namespace "$namespace" -o wide)
EOF

        if [[ $namespace == "elasticsearch" ]]
        then
            cat <<EOF
$ curl --silent -XGET http://172.16.103.101:9200/_cluster/health?pretty
$(curl --silent -XGET http://172.16.103.101:9200/_cluster/health?pretty)

$ curl --silent -XGET 'http://es.intr:9200/_cat/allocation?v'
$(curl --silent -XGET 'http://es.intr:9200/_cat/allocation?v')

$ elasticsearch shards
$(elasticsearch shards)

# logs
ssh -q fluentd.intr sudo tail -f /home/jenkins/es-curator/curator.log
ssh -q kvm15.intr journalctl -u elasticsearch.service -f | grep -vF '[max_concurrent_shard_requests] is not supported in the metadata section and will be rejected in 7.x'
ssh -q fluentd.intr sudo docker logs --tail 10 -f elk_elasticsearch_1 | grep -vF '[max_concurrent_shard_requests] is not supported in the metadata section and will be rejected in 7.x'
ssh -q staff.intr journalctl -u elasticsearch.service -f | grep -vF '[max_concurrent_shard_requests] is not supported in the metadata section and will be rejected in 7.x'
EOF
        fi

        if [[ $namespace == "opensearch" ]]
        then
            cat <<EOF

$ curl --insecure --silent --user "admin:PASSWORD" "${OPENSEARCH_ENDPOINT}/_cat/indices?v&pretty"
$(command curl --insecure --silent --user "admin:${OPENSEARCH_PASSWORD}" "${OPENSEARCH_ENDPOINT}/_cat/indices?v&pretty" | awk 'NR<3{print $0;next}{print $0| "sort --key=2 --version-sort"}')

$ curl --insecure --silent --user "admin:PASSWORD" "${OPENSEARCH_ENDPOINT}/_cat/shards?pretty"
$(command curl --insecure --silent --user "admin:${OPENSEARCH_PASSWORD}" "${OPENSEARCH_ENDPOINT}/_cat/shards?pretty" | awk 'NR<3{print $0;next}{print $0| "sort --version-sort"}')

$ curl --header 'Content-Type: application/json' --insecure --silent --user "admin:PASSWORD" --data '{"query": {"match_all": {}}}' "${OPENSEARCH_ENDPOINT}/logstash-*/_search?pretty" | jq --raw-output '.hits.hits[] | ._source.log'
$(command curl --header 'Content-Type: application/json' --insecure --silent --user "admin:${OPENSEARCH_PASSWORD}" --data '{"query": {"match_all": {}}}' "${OPENSEARCH_ENDPOINT}/logstash-*/_search?pretty" | jq --raw-output '.hits.hits[] | ._source.log')
EOF
        fi

        if [[ $namespace == "redis" ]]
        then
            cat <<EOF
$ dig redis.intr
$(bash -ic "dig redis.intr 2>&1")

Listing All Databases

In the first place, the number of databases in Redis is fixed. Therefore, we
can extract this information from the configuration file with a simple grep
command:

$ redis-cli -h redis.intr CONFIG GET databases
$(redis-cli -h redis.intr CONFIG GET databases)

Listing All Databases With Entries

Sometimes we'll want to get more information about the databases that contain
keys. In order to do that, we can take advantage of the Redis INFO command,
used to get information and statistics about the server. Here, we specifically
want to focus our attention in the keyspace section, which contains
database-related data:

$ redis-cli -h redis.intr INFO keyspace
$(redis-cli -h redis.intr INFO keyspace)

The output lists the databases containing at least one key, along with a few
statistics:

- number of keys contained
- number of keys with expiration
- keys' average time-to-live

$ redis-cli -h redis.intr INFO
$(redis-cli -h redis.intr INFO)

$ redis-cli -h redis.intr CLIENT LIST
$(redis-cli -h redis.intr CLIENT LIST)
EOF
        fi

        if [[ $namespace == "nfs" ]]
        then
            cat <<EOF
$ dig nfs.intr
$(bash -ic "dig nfs.intr 2>&1")

$ ssh root@kube6.intr showmount -e
$(ssh root@kube6.intr showmount -e)

$ ssh root@kube6.intr showmount
$(ssh root@kube6.intr showmount)

$ ssh root@kube6.intr rpcinfo
$(ssh root@kube6.intr rpcinfo)

$ ssh root@kube6.intr rpcinfo -p
$(ssh root@kube6.intr rpcinfo -p)

$ ssh root@kube6.intr nfsstat
$(ssh root@kube6.intr nfsstat)

$ ssh root@kube6.intr nfsstat -m
$(ssh root@kube6.intr nfsstat -m)

EOF
        fi
    }
    export -f viddy_command
    viddy_args=(
        --no-title
        --interval "${KUBECTL_WATCH_INTERVAL:-10}"
    )
    # Upstream viddy does not have max-history flag.
    if [[ $(viddy --help) == *max-history* ]]
    then
        viddy_args+=("--max-history=${KUBECTL_WATCH_HISTORY:-10}")
    fi
    tmux_window_name="$(tmux display-message -p '#W')"
    if [[ $tmux_window_name == bash ]]
    then
        tmux rename-window "$namespace"
    fi
    viddy "${viddy_args[@]}" viddy_command
    # XXX: Dirty hack and has issue that requires to run twice because of
    # racing with tmuxifier.
    if [[ $(tmux display-message -p '#W') == main ]]
    then
        tmux rename-window "bash"
    else
        tmux rename-window "$tmux_window_name"
    fi
fi
